import logging
import asyncio
import hashlib  # [New] í•´ì‹œ ê³„ì‚°ìš©
from app.core.queues import queues
from app.core.database import SessionLocal
from app.services.advisor_service import AdvisorService
from app.models.all_models import Incident, Job
from app.llm.models import (
    IncidentAnalysisRequest,
    EvidenceRef,
)  # [New] EvidenceRef ì„í¬íŠ¸

logger = logging.getLogger("llm_ctrl")


class LLMController:
    def __init__(self):
        self.advisor = AdvisorService()

    async def run_loop(self):
        logger.info("ğŸŸ£ LLM Advisor Controller Started")
        while True:
            item = await queues.llm_queue.get()
            try:
                # 1. Queue Item íŒŒì‹±
                analysis = item.get("analysis", {})
                meta = item.get("meta", {})
                record = item.get("record", {})

                # [ìˆ˜ì •] EvidenceRef ìƒì„± (ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°©ì§€)
                raw_line = record.get("raw_line", "")
                evidence = EvidenceRef(
                    type="raw",
                    ref_id=f"log_{int(asyncio.get_event_loop().time())}",
                    source=record.get("source_type", "unknown"),
                    offset=0,
                    length=len(raw_line),
                    # ê°„ë‹¨í•œ SHA256 ê³„ì‚° (í•„ìˆ˜ í•„ë“œ)
                    sha256=hashlib.sha256(raw_line.encode("utf-8")).hexdigest(),
                    rule_id="detect_module",
                )

                # 2. Request ìƒì„± (ì¦ê±° í¬í•¨)
                req = IncidentAnalysisRequest(
                    incident_id=f"inc-{item.get('agent_id')}-{int(asyncio.get_event_loop().time())}",
                    event_text=f"Threat detected on {meta.get('host')}. Score: {analysis.get('max_score')}\nLog: {raw_line[:200]}",
                    evidences=[evidence],  # [ìˆ˜ì •] ìƒì„±í•œ ì¦ê±° ê°ì²´ ì „ë‹¬
                )

                # 3. Advisor ë¶„ì„ ì‹¤í–‰
                result = await self.advisor.analyze(req)

                # 4. ê²°ê³¼ ì €ì¥
                with SessionLocal() as db:
                    self._save(db, item, result)

            except Exception as e:
                logger.error(f"LLM Error: {e}")
            finally:
                queues.llm_queue.task_done()

    def _save(self, db, item, result):
        inc = Incident(
            client_id=item["meta"]["client_id"],
            summary=result.summary,
            status="new" if result.status == "pending_approval" else "active",
            recommended_actions=[{"action": a} for a in result.recommended_actions],
            confidence=int(result.confidence * 100),
            incident_metadata={
                "attack_mapping": result.attack_mapping,
                "hil_required": result.hil_required,
            },
        )
        db.add(inc)

        # ìë™ ëŒ€ì‘ Job ìƒì„± (ìŠ¹ì¸ëœ ê²½ìš°)
        if result.status == "approved":
            job = Job(
                client_id=item["meta"]["client_id"],
                agent_id=item["agent_id"],
                job_type="BLOCK_IP",
                args={"reason": "LLM Auto-Response"},
                status="ready",
            )
            db.add(job)
            logger.info(f"âš¡ Auto-Response Job Created: {job.job_id}")

        db.commit()
        logger.info(f"âœ… Incident Created: {inc.incident_id}")
