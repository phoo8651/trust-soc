#llm/local_llm_PoC.py
import os
import json
import asyncio
import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

DEFAULT_MISTRAL_MODEL = os.getenv(
  "LOCAL_MODEL",
  os.path.join("llm", "models", "mistral-7b-instruct-v0.2.Q4_K_M.gguf"),
)

# -----------------------------------------------------
# â‘  DummyLocalLLM: ê°œë°œìš© ëª¨ì˜ ì‘ë‹µ
# -----------------------------------------------------
class DummyLocalLLM:
    def __init__(self, model_path: Optional[str] = None):
        self.model_path = model_path
        logger.info(f"[DummyLocalLLM] initialized (model_path={self.model_path})")

    def generate(self, prompt: str) -> str:
        logger.info("[DummyLocalLLM] generating mock response")
        parsed: Dict[str, Any] = {
            "summary": "ëª¨ì˜ ìš”ì•½ (dummy)",
            "attack_mapping": ["T1595"],
            "recommended_actions": ["ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ê°•í™”"],
            "confidence": 0.5,
            "evidence_refs": [
                {
                    "type": "raw",
                    "ref_id": "log_001",
                    "source": "auth.log",
                    "offset": 0,
                    "length": 100,
                    "sha256": (
                        "abcdef1234567890abcdef1234567890"
                        "abcdef1234567890abcdef1234567890"
                    ),
                }
            ],
            "hil_required": False,
        }
        return json.dumps(parsed, ensure_ascii=False)


# -----------------------------------------------------
# â‘¡ LocalMistralLLM: mistral-7b-instruct GGUF ë¡œì»¬ ëª¨ë¸ ì—°ê²°
# -----------------------------------------------------
class LocalMistralLLM:
    def __init__(self, model_path: Optional[str] = None):
        from llama_cpp import Llama  # ëŸ°íƒ€ì„ ì—”ì§„ (GGUFìš©)

        # ê²½ë¡œê°€ ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ Mistral ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        self.model_path = model_path or DEFAULT_MISTRAL_MODEL

        self.llm = Llama(
            model_path=self.model_path,
            n_ctx=1024,   # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            n_threads=4,  # CPU ìŠ¤ë ˆë“œ ìˆ˜ (í™˜ê²½ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥)
            verbose=False
        )

    def generate(self, prompt, max_tokens: int = 256, temperature: float = 0.0):
        logger.info("[LocalMistralLLM] Generating with mistral-7b-instruct...")
        try:
            output = self.llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=1.0,
                stop=["}"],
            )
        except Exception as e:
            logger.error(f"ğŸ”¥ LocalMistralLLM crashed: {e}")
            raise
        
        return output["choices"][0]["text"]
        #return output["choices"][0]["text"].strip()


