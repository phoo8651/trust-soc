import os
import json
import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

DEFAULT_MISTRAL_MODEL = os.getenv(
    "LOCAL_MODEL",
    os.path.join("llm", "models", "mistral-7b-instruct-v0.2.Q4_K_M.gguf"),
)


class DummyLocalLLM:
    def __init__(self, model_path: Optional[str] = None):
        self.model_path = model_path
        logger.info(f"[DummyLocalLLM] initialized (model_path={self.model_path})")

    def generate(self, prompt: str) -> str:
        logger.info("[DummyLocalLLM] generating mock response")
        # (ê¸°ì¡´ ë”ë¯¸ ë¡œì§ ìœ ì§€)
        parsed: Dict[str, Any] = {
            "summary": "ëª¨ì˜ ìš”ì•½ (dummy)",
            "attack_mapping": ["T1595"],
            "recommended_actions": ["ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ê°•í™”"],
            "confidence": 0.5,
            "evidence_refs": [],
            "hil_required": False,
        }
        return json.dumps(parsed, ensure_ascii=False)


class LocalMistralLLM:
    def __init__(self, model_path: Optional[str] = None):
        from llama_cpp import Llama

        self.model_path = model_path or DEFAULT_MISTRAL_MODEL

        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"Model not found: {self.model_path}")

        # [ìˆ˜ì • 1] ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì¦ê°€ (ì…ë ¥+ì¶œë ¥ í•©ê³„ ìš©ëŸ‰)
        # ê¸°ì¡´ 1024 -> 4096 (Mistral ëª¨ë¸ì˜ ì—¬ìœ  ê³µê°„ í™•ë³´)
        self.llm = Llama(
            model_path=self.model_path, n_ctx=4096, n_gpu_layers=-1, verbose=False
        )

    def generate(self, prompt: str) -> str:
        # [ìˆ˜ì • 2] ìƒì„± ìµœëŒ€ ê¸¸ì´ ì¦ê°€ (ì¶œë ¥ ìš©ëŸ‰)
        # ê¸°ì¡´ 256 -> 2048 (ê¸´ ë³´ê³ ì„œë„ ì˜ë¦¬ì§€ ì•Šë„ë¡ ì¶©ë¶„íˆ í™•ë³´)
        try:
            output = self.llm(
                prompt,
                max_tokens=2048,
                temperature=0.1,
                top_p=0.95,
                stop=["</s>", "END_JSON"],  # ì¢…ë£Œ ì¡°ê±´ ëª…í™•í™”
                echo=False,
            )
            return output["choices"][0]["text"]
        except Exception as e:
            logger.error(f"ğŸ”¥ LocalMistralLLM crash: {e}")
            raise
