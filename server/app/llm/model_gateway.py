import time
import logging
import hashlib
import asyncio
import traceback  # [New] ìƒì„¸ ì—ëŸ¬ ë¡œê·¸ë¥¼ ìœ„í•´ ì¶”ê°€
from typing import Dict, Any

# íŒ¨í‚¤ì§€ ê²½ë¡œ í™•ì¸
from app.llm.local_llm_PoC import DummyLocalLLM, LocalMistralLLM

logger = logging.getLogger("ModelGateway")


class ModelGateway:
    def __init__(
        self,
        local_model_path: str,
        use_real_llm: bool = True,
        enable_fallback: bool = True,
        timeout: float = 180,  # [ìˆ˜ì •] íƒ€ì„ì•„ì›ƒ 180ì´ˆ(3ë¶„)ë¡œ ë„‰ë„‰í•˜ê²Œ ì¦ê°€
    ):
        self.timeout = timeout
        self.enable_fallback = enable_fallback
        self.mock_mode = False

        if use_real_llm:
            try:
                self.llm = LocalMistralLLM(model_path=local_model_path)
                logger.info(f"âœ… Real LLM Loaded: {local_model_path}")
            except Exception as e:
                logger.error(f"âŒ Failed to load Real LLM: {e}")
                logger.error(traceback.format_exc())  # ë¡œë”© ì‹¤íŒ¨ ì‹œì—ë„ ìƒì„¸ ë¡œê·¸ ì¶œë ¥

                if not enable_fallback:
                    raise e

                logger.warning("âš ï¸ Switching to Dummy LLM due to load failure.")
                self.llm = DummyLocalLLM()
                self.mock_mode = True
        else:
            self.llm = DummyLocalLLM()
            self.mock_mode = True

    async def generate(self, prompt: str) -> str:
        start_time = time.time()
        try:
            if self.mock_mode:
                return self.llm.generate(prompt)

            logger.info("â³ Sending prompt to Local LLM...")

            # [ìˆ˜ì •] íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ ëª…í™•íˆ ì¡ê¸° ìœ„í•´ wait_for ì‚¬ìš©
            return await asyncio.wait_for(
                asyncio.to_thread(self.llm.generate, prompt), timeout=self.timeout
            )

        except asyncio.TimeoutError:
            # [New] íƒ€ì„ì•„ì›ƒ ì—ëŸ¬ ë³„ë„ ì²˜ë¦¬
            logger.error(f"â° LLM Timeout! Execution took longer than {self.timeout}s")
            if self.enable_fallback:
                return self._fallback_response()
            raise TimeoutError("LLM Generation Timed Out")

        except Exception as e:
            # [New] ì¼ë°˜ ì—ëŸ¬ ë°œìƒ ì‹œ Traceback ì „ì²´ ì¶œë ¥
            error_msg = str(e)
            stack_trace = traceback.format_exc()

            logger.error(f"âŒ Local LLM Runtime Error: {error_msg}")
            logger.error(f"ğŸ” Stack Trace:\n{stack_trace}")

            if self.enable_fallback:
                logger.info("ğŸ”„ Activating Fallback Mechanism (Dummy Response)")
                return self._fallback_response()
            raise e

        finally:
            duration = time.time() - start_time
            logger.info(f"â±ï¸ LLM Processing Time: {duration:.2f}s")

    def _fallback_response(self):
        """Fallback ì‹œ ì‚¬ìš©í•  ë”ë¯¸ ì‘ë‹µ ìƒì„±"""
        return DummyLocalLLM().generate("fallback")
