#llm/model_gateway.py
import time
import logging
import hashlib
import asyncio
import os

from typing import Dict, Any, Optional
from llm.local_llm_PoC import DummyLocalLLM

# ë¡œì»¬ LLM ëª¨ë“ˆ (llama.cpp ê¸°ë°˜)
from llm.local_llm_PoC import LocalLlamaLLM

# ë¡œê·¸ ì„¤ì •
logger = logging.getLogger("ModelGateway")
logger.setLevel(logging.INFO)


class ModelGateway:
    """
    LLM í˜¸ì¶œ ê²Œì´íŠ¸ì›¨ì´
    - ê¸°ë³¸: ë¡œì»¬ LLM(gguf) ì‹¤í–‰
    - ì‹¤íŒ¨ ì‹œ fallback ë”ë¯¸ ëª¨ë¸ í˜¸ì¶œ ê°€ëŠ¥
    - ëª¨ë¸ í˜¸ì¶œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§(metrics logging)
    """

    def __init__(
        self,
        local_model_path: str,   # GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        use_real_llm: bool = True,  # ì‹¤ì œ LLM ì‚¬ìš©í• ì§€ ì—¬ë¶€
        enable_fallback: bool = True,  # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ëª¨ë¸ fallback
        monitoring_enabled: bool = True,  # ì„±ëŠ¥ ë¡œê·¸ ê¸°ë¡ ì—¬ë¶€
        timeout: float = 60  # ìµœëŒ€ ì‘ë‹µ ëŒ€ê¸° ì‹œê°„
    ):
        self.timeout = timeout
        self.enable_fallback = enable_fallback
        self.monitoring_enabled = monitoring_enabled
        self.mock_mode = False
        self.cache_enabled = False #ìºì‹œ ë¹„í™œì„±í™” ìƒíƒœ
        self.cache: Dict[str, Any] = {}


        # ì‹¤ì œ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¡œë“œ
        if use_real_llm:
            logger.info(f"ğŸ”¹ Local LLM ëª¨ë¸ ë¡œë“œ: {local_model_path}")
            # ëª¨ë¸ íŒŒì¼ ìœ íš¨í•œì§€ í™•ì¸
            if not local_model_path or not os.path.exists(local_model_path):
                logger.warning("ğŸ§ª No local model found â†’ DummyLLM fallback")
                from llm.local_llm_PoC import DummyLocalLLM
                self.llm = DummyLocalLLM()
            else:
                self.llm = LocalLlamaLLM(model_path=local_model_path)
        else:
            logger.info("âš™ DummyLocalLLM ì‚¬ìš©")
            from llm.local_llm_PoC import DummyLocalLLM
            self.llm = DummyLocalLLM()

    # ---------------------------------------------------------
    #  ëª¨ë¸ í˜¸ì¶œ í•¨ìˆ˜ (ë¹„ë™ê¸°)
    # ---------------------------------------------------------
    async def generate(self, prompt: str) -> str:
        start = time.time()
        cache_key = hashlib.sha256(prompt.encode()).hexdigest()

        try:
            # ìºì‹œ Hit
            if self.cache_enabled and cache_key in self.cache:
                return self.cache[cache_key]

            # Mock ëª¨ë“œ
            if self.mock_mode:
                output = '{"summary": "Mock summary for test", "confidence": 0.5}'
            else:
                # Thread + Timeout ì ìš©
                output = await asyncio.wait_for(
                    asyncio.to_thread(self.llm.generate, prompt),
                    timeout=self.timeout
                )

            if self.cache_enabled:
                self.cache[cache_key] = output

        except Exception as e:
            logger.warning(f"âŒ Local LLM ì‹¤í–‰ ì‹¤íŒ¨: {e}")

            if not self.enable_fallback:
                raise

            logger.info("âš  Dummy ëª¨ë¸ë¡œ Fallback ì²˜ë¦¬")
            from llm.local_llm_PoC import DummyLocalLLM
            dummy = DummyLocalLLM()
            output = dummy.generate(prompt)

        duration = time.time() - start
        if self.monitoring_enabled:
            self.log_metrics(tokens_used=len(prompt), duration=duration)

        return output



    # ---------------------------------------------------------
    #  Metric Logging (í† í°ìˆ˜ ë° ì‘ë‹µ ì‹œê°„ ì¸¡ì •)
    # ---------------------------------------------------------
    def log_metrics(self, tokens_used: int, duration: float):
        logger.info(f"ğŸ“Š [Metrics] ì‚¬ìš© í† í°ìˆ˜={tokens_used}, ì‘ë‹µì‹œê°„={duration:.2f}ì´ˆ")
