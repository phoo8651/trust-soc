import time
import logging
from typing import Dict, Any, Optional

# ë¡œì»¬ LLM ëª¨ë“ˆ (llama.cpp ê¸°ë°˜)
from llm.local_llm_PoC import LocalLlamaLLM

# ë¡œê·¸ ì„¤ì •
logger = logging.getLogger("ModelGateway")
logger.setLevel(logging.INFO)


class ModelGateway:
    """
    LLM í˜¸ì¶œ ê²Œì´íŠ¸ì›¨ì´
    - ê¸°ë³¸: ë¡œì»¬ LLM(gguf) ì‹¤í–‰
    - ì‹¤íŒ¨ ì‹œ fallback ë”ë¯¸ ëª¨ë¸ í˜¸ì¶œ ê°€ëŠ¥
    - ëª¨ë¸ í˜¸ì¶œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§(metrics logging)
    """

    def __init__(
        self,
        local_model_path: str,   # GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        use_real_llm: bool = True,  # ì‹¤ì œ LLM ì‚¬ìš©í• ì§€ ì—¬ë¶€
        enable_fallback: bool = True,  # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ëª¨ë¸ fallback
        monitoring_enabled: bool = True,  # ì„±ëŠ¥ ë¡œê·¸ ê¸°ë¡ ì—¬ë¶€
        timeout: float = 20  # ìµœëŒ€ ì‘ë‹µ ëŒ€ê¸° ì‹œê°„
    ):
        self.timeout = timeout
        self.enable_fallback = enable_fallback
        self.monitoring_enabled = monitoring_enabled

        # ì‹¤ì œ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¡œë“œ
        if use_real_llm:
            logger.info(f"ğŸ”¹ Local LLM ëª¨ë¸ ë¡œë“œ: {local_model_path}")
            self.llm = LocalLlamaLLM(model_path=local_model_path)
        else:
            logger.info("âš™ DummyLocalLLM ì‚¬ìš©")
            from llm.local_llm_PoC import DummyLocalLLM
            self.llm = DummyLocalLLM()

    # ---------------------------------------------------------
    #  ëª¨ë¸ í˜¸ì¶œ í•¨ìˆ˜ (ë¹„ë™ê¸°)
    # ---------------------------------------------------------
    async def generate(self, prompt: str) -> str:
        """
        LLM ëª¨ë¸ í˜¸ì¶œ
        - ì˜¤ë¥˜ ë°œìƒ ì‹œ fallback ì²˜ë¦¬
        """
        start = time.time()

        try:
            output = self.llm.generate(prompt)

        except Exception as e:
            logger.warning(f"âŒ Local LLM ì‹¤í–‰ ì‹¤íŒ¨: {e}")

            if not self.enable_fallback:
                raise

            # fallback: DummyLocalLLM ì‚¬ìš©
            logger.info("âš  Dummy ëª¨ë¸ë¡œ Fallback ì²˜ë¦¬")
            from llm.local_llm_PoC import DummyLocalLLM
            dummy = DummyLocalLLM()
            output = dummy.generate(prompt)

        # ì„±ëŠ¥ ë¡œê·¸ ê¸°ë¡
        duration = time.time() - start
        if self.monitoring_enabled:
            self.log_metrics(tokens_used=len(prompt), duration=duration)

        return output

    # ---------------------------------------------------------
    #  Metric Logging (í† í°ìˆ˜ ë° ì‘ë‹µ ì‹œê°„ ì¸¡ì •)
    # ---------------------------------------------------------
    def log_metrics(self, tokens_used: int, duration: float):
        logger.info(f"ğŸ“Š [Metrics] ì‚¬ìš© í† í°ìˆ˜={tokens_used}, ì‘ë‹µì‹œê°„={duration:.2f}ì´ˆ")
