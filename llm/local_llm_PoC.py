# llm/locol_llm.PoC.py
from llama_cpp import Llama


MODEL_PATH = "C:/Users/ngh11/trust-soc/llm/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf"

# ëª¨ë¸ ë¡œë“œ
llm = Llama(model_path=MODEL_PATH, n_ctx=2048, n_threads=6)

# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
prompt = """SYSTEM:
ë‹¹ì‹ ì€ ë³´ì•ˆ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ëŠ” AIì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¡œê·¸ì˜ ê³µê²© ì§•í›„ë¥¼ ìš”ì•½í•˜ì„¸ìš”.

USER:
[EVENT]
2025-11-02 14:11:33 - POST /app/login - suspicious payload detected

[TOP-K EVIDENCES]
ref_id: log_001
type: raw
source: web_log
snippet: "payload contains eval() function"

OUTPUT_SCHEMA:
{
  "summary": "",
  "confidence_explanation": "",
  "evidence_refs_used": []
}
"""

print("ğŸ§  LLM ì‘ë‹µ ìƒì„± ì¤‘...")
response = llm(prompt=prompt, max_tokens=256, temperature=0.7, stop=["</s>"])
print(response["choices"][0]["text"])

import json

class DummyLocalLLM:
    def __init__(self, model_path: str = None):
        self.model_path = model_path
        print(f"[DummyLocalLLM] model_path: {self.model_path}")
        
    def generate(self, prompt: str) -> str:
        return json.dumps({
        "summary": "ëª¨ì˜ ìš”ì•½",
        "attack_mapping": ["ëª¨ë¦„"],
        "recommended_actions": ["ëª¨ë¦„"],
        "confidence": 0.5,
        "evidence_refs": [{"type":"raw","ref_id":"log_001","source":"auth.log","offset":0,"length":150,"sha256":"abc123"}],
        "hil_required": False
    })

    '''def generate(self, prompt: str) -> str:
        # ë‹¨ìˆœíˆ í”„ë¡¬í”„íŠ¸ë¥¼ echo í•˜ëŠ” PoC ì˜ˆì‹œ
        return '{"summary": "ëª¨ì˜ ìš”ì•½", "attack_mapping": ["ëª¨ë¦„"], "recommended_actions": ["ëª¨ë¦„"], "confidence": 0.5, "hil_required": false}'
    '''